{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from localgraphclustering import *\n",
    "except:\n",
    "    # when the package is not installed, import the local version instead. \n",
    "    # the notebook must be placed in the original \"notebooks/\" folder\n",
    "    sys.path.append(\"../\")\n",
    "    from localgraphclustering import * \n",
    "\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import random\n",
    "\n",
    "import statistics as stat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/localgraphclustering/GraphLocal.py:222: UserWarning:\n",
      "\n",
      "Loading a graphml is not efficient, we suggest using an edgelist format for this API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GraphLocal('../datasets/Colgate88_reduced.graphml','graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('../datasets/Colgate88_reduced.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id:  secondMajor0\n",
      "Cluster:  1  conductance:  0.5445356589319748 Size:  2107  Volume:  175239.0\n",
      "Id:  gender1\n",
      "Cluster:  3  conductance:  0.485824051260122 Size:  1695  Volume:  162759.0\n",
      "Id:  gender2\n",
      "Cluster:  9  conductance:  0.5516795447932495 Size:  1485  Volume:  123724.0\n",
      "Id:  year2009\n",
      "Cluster:  12  conductance:  0.11998643262952599 Size:  641  Volume:  35379.0\n",
      "Id:  year2007\n",
      "Cluster:  17  conductance:  0.41601591061975374 Size:  589  Volume:  68382.0\n",
      "Id:  dorm0\n",
      "Cluster:  22  conductance:  0.5394666082418786 Size:  1157  Volume:  100414.0\n",
      "Id:  year2005\n",
      "Cluster:  23  conductance:  0.5011748909029876 Size:  501  Volume:  50643.0\n",
      "Id:  year2008\n",
      "Cluster:  40  conductance:  0.29312830370014414 Size:  641  Volume:  62430.0\n",
      "Id:  year2004\n",
      "Cluster:  47  conductance:  0.5421816227834497 Size:  230  Volume:  14888.0\n",
      "Id:  year2006\n",
      "Cluster:  71  conductance:  0.488052847820833 Size:  557  Volume:  62065.0\n"
     ]
    }
   ],
   "source": [
    "ground_truth_clusters_by_number = dict()\n",
    "\n",
    "cluster_names = ['secondMajor','highSchool','gender','dorm','majorIndex','year']\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    \n",
    "    for cluster_name in cluster_names:\n",
    "        \n",
    "        ground_truth_clusters_by_number[cluster_name+str(node[1][cluster_name])] =  []\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    \n",
    "    for cluster_name in cluster_names:\n",
    "        \n",
    "        ground_truth_clusters_by_number[cluster_name+str(node[1][cluster_name])].append(int(node[0]))\n",
    "        \n",
    "all_clusters = []\n",
    "counter = 0\n",
    "for cluster_id in ground_truth_clusters_by_number:\n",
    "    \n",
    "    cluster = ground_truth_clusters_by_number[cluster_id]\n",
    "    \n",
    "    if len(cluster) == 1 or len(cluster) == 0:\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "#     eig, lambda_ = fiedler_local(g, cluster)\n",
    "#     lambda_ = np.real(lambda_)\n",
    "#     gap = lambda_/g.compute_conductance(cluster)\n",
    "    cond = g.compute_conductance(cluster)\n",
    "    counter += 1\n",
    "    \n",
    "    if cond <= 0.57 and len(cluster) >= 10:\n",
    "        print(\"Id: \", cluster_id)\n",
    "        print(\"Cluster: \", counter, \" conductance: \", cond, \"Size: \", len(cluster), \" Volume: \", np.sum(g.d[cluster]))\n",
    "        all_clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for ACL (with rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_acl = {}\n",
    "external_best_pre_cond_acl = {}\n",
    "vol_best_cond_acl = {}\n",
    "vol_best_pre_acl = {}\n",
    "size_clust_best_cond_acl = {}\n",
    "size_clust_best_pre_acl = {}\n",
    "f1score_best_cond_acl = {}\n",
    "f1score_best_pre_acl = {}\n",
    "true_positives_best_cond_acl = {}\n",
    "true_positives_best_pre_acl = {}\n",
    "precision_best_cond_acl = {}\n",
    "precision_best_pre_acl = {}\n",
    "recall_best_cond_acl = {}\n",
    "recall_best_pre_acl = {}\n",
    "cuts_best_cond_acl = {}\n",
    "cuts_best_pre_acl = {}\n",
    "cuts_acl_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    eigv, lambda_val = fiedler_local(g, rr)\n",
    "    lambda_val = np.real(lambda_val)\n",
    "    \n",
    "    step = (2*lambda_val - lambda_val/2)/4\n",
    "    \n",
    "    a_list = np.arange(lambda_val/2,2*lambda_val,step)\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "        \n",
    "        ct_inner = 0\n",
    "        for a in a_list:\n",
    "            \n",
    "            if ct_outer <= 1:\n",
    "                rho = 0.15/np.sum(g.d[rr])\n",
    "            else:\n",
    "                rho = 0.2/np.sum(g.d[rr])\n",
    "            \n",
    "            output_pr_clustering = approximate_PageRank(g,ref_node,method = \"acl\", rho=rho, alpha=a, cpp = True, normalize=True,normalized_objective=True)\n",
    "            number_experiments += 1\n",
    "            \n",
    "            output_pr_sc = sweep_cut(g,output_pr_clustering,cpp=True)\n",
    "            \n",
    "            S = output_pr_sc[0]\n",
    "            \n",
    "            cuts_acl_ALL[ct_outer,node,ct_inner] = S\n",
    "            \n",
    "            size_clust_acl_ = len(S)\n",
    "            \n",
    "            cond_val_l1pr = g.compute_conductance(S)\n",
    "            \n",
    "            vol_ = sum(g.d[S])\n",
    "            true_positives_acl_ = set(rr).intersection(S)\n",
    "            if len(true_positives_acl_) == 0:\n",
    "                true_positives_acl_ = set(ref_node)\n",
    "                vol_ = g.d[ref_node][0,0]\n",
    "            precision = sum(g.d[np.array(list(true_positives_acl_))])/vol_\n",
    "            recall = sum(g.d[np.array(list(true_positives_acl_))])/sum(g.d[rr])\n",
    "            f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if f1_score_ >= max_precision:\n",
    "                \n",
    "                max_precision = f1_score_\n",
    "                \n",
    "                external_best_pre_cond_acl[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_pre_acl[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_pre_acl[ct_outer,node] = size_clust_acl_\n",
    "                true_positives_best_pre_acl[ct_outer,node] = true_positives_acl_\n",
    "                precision_best_pre_acl[ct_outer,node] = precision\n",
    "                recall_best_pre_acl[ct_outer,node] = recall\n",
    "                f1score_best_pre_acl[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_pre_acl[ct_outer,node] = S\n",
    "        \n",
    "            if cond_val_l1pr <= min_conduct:\n",
    "                \n",
    "                min_conduct = cond_val_l1pr\n",
    "                \n",
    "                external_best_cond_acl[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_cond_acl[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_cond_acl[ct_outer,node] = size_clust_acl_\n",
    "                true_positives_best_cond_acl[ct_outer,node] = true_positives_acl_\n",
    "                precision_best_cond_acl[ct_outer,node] = precision\n",
    "                recall_best_cond_acl[ct_outer,node] = recall\n",
    "                f1score_best_cond_acl[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_cond_acl[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_acl[ct_outer,node], 'f1score: ', f1score_best_cond_acl[ct_outer,node], 'precision: ', precision_best_cond_acl[ct_outer,node], 'recall: ', recall_best_cond_acl[ct_outer,node])\n",
    "        ct += 1\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time ACL with rounding: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of ACL (with rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for ACL with rounding')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_acl[i,j])\n",
    "        temp_rec.append(recall_best_cond_acl[i,j])\n",
    "        temp_f1.append(f1score_best_cond_acl[i,j])\n",
    "        temp_conductance.append(external_best_cond_acl[i,j])\n",
    "    \n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for l1-reg. PR (with rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_acl = {}\n",
    "external_best_pre_cond_acl = {}\n",
    "vol_best_cond_acl = {}\n",
    "vol_best_pre_acl = {}\n",
    "size_clust_best_cond_acl = {}\n",
    "size_clust_best_pre_acl = {}\n",
    "f1score_best_cond_acl = {}\n",
    "f1score_best_pre_acl = {}\n",
    "true_positives_best_cond_acl = {}\n",
    "true_positives_best_pre_acl = {}\n",
    "precision_best_cond_acl = {}\n",
    "precision_best_pre_acl = {}\n",
    "recall_best_cond_acl = {}\n",
    "recall_best_pre_acl = {}\n",
    "cuts_best_cond_acl = {}\n",
    "cuts_best_pre_acl = {}\n",
    "cuts_acl_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    eigv, lambda_val = fiedler_local(g, rr)\n",
    "    lambda_val = np.real(lambda_val)\n",
    "    \n",
    "    step = (2*lambda_val - lambda_val/2)/4\n",
    "    \n",
    "    a_list = np.arange(lambda_val/2,2*lambda_val,step)\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "        \n",
    "        ct_inner = 0\n",
    "        for a in a_list:\n",
    "            \n",
    "            if ct_outer <= 1:\n",
    "                rho = 0.15/np.sum(g.d[rr])\n",
    "            else:\n",
    "                rho = 0.2/np.sum(g.d[rr])\n",
    "            \n",
    "            output_pr_clustering = approximate_PageRank(g,ref_node,method = \"l1reg-rand\", epsilon=1.0e-6, rho=rho, alpha=a, cpp = True, normalize=True,normalized_objective=True)\n",
    "            number_experiments += 1\n",
    "            \n",
    "            output_pr_sc = sweep_cut(g,output_pr_clustering,cpp=True)\n",
    "            \n",
    "            S = output_pr_sc[0]\n",
    "            \n",
    "            cuts_acl_ALL[ct_outer,node,ct_inner] = S\n",
    "            \n",
    "            size_clust_acl_ = len(S)\n",
    "            \n",
    "            cond_val_l1pr = g.compute_conductance(S)\n",
    "            \n",
    "            vol_ = sum(g.d[S])\n",
    "            true_positives_acl_ = set(rr).intersection(S)\n",
    "            if len(true_positives_acl_) == 0:\n",
    "                true_positives_acl_ = set(ref_node)\n",
    "                vol_ = g.d[ref_node][0,0]\n",
    "            precision = sum(g.d[np.array(list(true_positives_acl_))])/vol_\n",
    "            recall = sum(g.d[np.array(list(true_positives_acl_))])/sum(g.d[rr])\n",
    "            f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if f1_score_ >= max_precision:\n",
    "                \n",
    "                max_precision = f1_score_\n",
    "                \n",
    "                external_best_pre_cond_acl[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_pre_acl[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_pre_acl[ct_outer,node] = size_clust_acl_\n",
    "                true_positives_best_pre_acl[ct_outer,node] = true_positives_acl_\n",
    "                precision_best_pre_acl[ct_outer,node] = precision\n",
    "                recall_best_pre_acl[ct_outer,node] = recall\n",
    "                f1score_best_pre_acl[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_pre_acl[ct_outer,node] = S\n",
    "        \n",
    "            if cond_val_l1pr <= min_conduct:\n",
    "                \n",
    "                min_conduct = cond_val_l1pr\n",
    "                \n",
    "                external_best_cond_acl[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_cond_acl[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_cond_acl[ct_outer,node] = size_clust_acl_\n",
    "                true_positives_best_cond_acl[ct_outer,node] = true_positives_acl_\n",
    "                precision_best_cond_acl[ct_outer,node] = precision\n",
    "                recall_best_cond_acl[ct_outer,node] = recall\n",
    "                f1score_best_cond_acl[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_cond_acl[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_acl[ct_outer,node], 'f1score: ', f1score_best_cond_acl[ct_outer,node], 'precision: ', precision_best_cond_acl[ct_outer,node], 'recall: ', recall_best_cond_acl[ct_outer,node])\n",
    "        ct += 1\n",
    "\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time l1-reg. with rounding: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of l1-reg. PR (with rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for l1-reg with rounding')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_acl[i,j])\n",
    "        temp_rec.append(recall_best_cond_acl[i,j])\n",
    "        temp_f1.append(f1score_best_cond_acl[i,j])\n",
    "        temp_conductance.append(external_best_cond_acl[i,j])\n",
    "    \n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for seed set expansion using BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "def seed_grow_bfs_steps(g,seeds,steps,vol_target,target_cluster):\n",
    "    \"\"\"\n",
    "    grow the initial seed set through BFS until its size reaches \n",
    "    a given ratio of the total number of nodes.\n",
    "    \"\"\"\n",
    "    Q = queue.Queue()\n",
    "    visited = np.zeros(g._num_vertices)\n",
    "    visited[seeds] = 1\n",
    "    for s in seeds:\n",
    "        Q.put(s)\n",
    "    if isinstance(seeds,np.ndarray):\n",
    "        seeds = seeds.tolist()\n",
    "    else:\n",
    "        seeds = list(seeds)\n",
    "    for step in range(steps):\n",
    "        for k in range(Q.qsize()):\n",
    "            node = Q.get()\n",
    "            si,ei = g.adjacency_matrix.indptr[node],g.adjacency_matrix.indptr[node+1]\n",
    "            neighs = g.adjacency_matrix.indices[si:ei]\n",
    "            for i in range(len(neighs)):\n",
    "                if visited[neighs[i]] == 0:\n",
    "                    visited[neighs[i]] = 1\n",
    "                    seeds.append(neighs[i])\n",
    "                    Q.put(neighs[i])\n",
    "                    \n",
    "                    vol_seeds = np.sum(g.d[seeds])\n",
    "                    vol_target_intersection_input = np.sum(g.d[list(set(target_cluster).intersection(set(seeds)))])\n",
    "                    sigma = vol_target_intersection_input/vol_target\n",
    "                    \n",
    "                    if sigma > 0.75 or vol_seeds > 0.25*g.vol_G:\n",
    "                        break\n",
    "                 \n",
    "            vol_seeds = np.sum(g.d[seeds])\n",
    "            vol_target_intersection_input = np.sum(g.d[list(set(target_cluster).intersection(set(seeds)))])\n",
    "            sigma = vol_target_intersection_input/vol_target   \n",
    "            \n",
    "            if sigma > 0.75 or vol_seeds > 0.25*g.vol_G:\n",
    "                break\n",
    "               \n",
    "        vol_seeds = np.sum(g.d[seeds])\n",
    "        vol_target_intersection_input = np.sum(g.d[list(set(target_cluster).intersection(set(seeds)))])\n",
    "        sigma = vol_target_intersection_input/vol_target\n",
    "                \n",
    "        if sigma > 0.75 or vol_seeds > 0.25*g.vol_G:\n",
    "            break\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for seed set expansion + FlowImprove, try a lot of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_flBFS = {}\n",
    "external_best_pre_cond_flBFS = {}\n",
    "vol_best_cond_flBFS = {}\n",
    "vol_best_pre_flBFS = {}\n",
    "size_clust_best_cond_flBFS = {}\n",
    "size_clust_best_pre_flBFS = {}\n",
    "f1score_best_cond_flBFS = {}\n",
    "f1score_best_pre_flBFS = {}\n",
    "true_positives_best_cond_flBFS = {}\n",
    "true_positives_best_pre_flBFS = {}\n",
    "precision_best_cond_flBFS = {}\n",
    "precision_best_pre_flBFS = {}\n",
    "recall_best_cond_flBFS = {}\n",
    "recall_best_pre_flBFS = {}\n",
    "cuts_best_cond_flBFS = {}\n",
    "cuts_best_pre_flBFS = {}\n",
    "cuts_flBFS_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    n_step = 24\n",
    "    \n",
    "    vol_target = np.sum(g.d[rr])\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "                \n",
    "        seeds = seed_grow_bfs_steps(g,[node],g._num_vertices,vol_target,rr)\n",
    "\n",
    "        vol_input = np.sum(g.d[seeds])\n",
    "\n",
    "        vol_graph_minus_input = np.sum(g.d[list(set(range(g._num_vertices)) - set(seeds))])\n",
    "\n",
    "        vol_target_intersection_input = np.sum(g.d[list(set(rr).intersection(set(seeds)))])\n",
    "\n",
    "        gamma = vol_input/vol_graph_minus_input\n",
    "                \n",
    "        sigma = max(vol_target_intersection_input/vol_target,gamma)\n",
    "        \n",
    "        delta = min(max((1/3)*(1.0/(1.0/sigma - 1)) - gamma,0),1)\n",
    "                        \n",
    "        S = flow_clustering(g,seeds,method=\"sl\",delta=delta)[0]\n",
    "        number_experiments += 1\n",
    "\n",
    "        cuts_flBFS_ALL[ct_outer,node] = S\n",
    "\n",
    "        size_clust_flBFS_ = len(S)\n",
    "\n",
    "        cond_val_l1pr = g.compute_conductance(S)\n",
    "\n",
    "        vol_ = sum(g.d[S])\n",
    "        true_positives_flBFS_ = set(rr).intersection(S)\n",
    "        if len(true_positives_flBFS_) == 0:\n",
    "            true_positives_flBFS_ = set(ref_node)\n",
    "            vol_ = g.d[ref_node][0]\n",
    "        precision = sum(g.d[np.array(list(true_positives_flBFS_))])/vol_\n",
    "        recall = sum(g.d[np.array(list(true_positives_flBFS_))])/sum(g.d[rr])\n",
    "        f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "        if f1_score_ >= max_precision:\n",
    "\n",
    "            max_precision = f1_score_\n",
    "\n",
    "            external_best_pre_cond_flBFS[ct_outer,node] = cond_val_l1pr\n",
    "            vol_best_pre_flBFS[ct_outer,node] = vol_\n",
    "\n",
    "            size_clust_best_pre_flBFS[ct_outer,node] = size_clust_flBFS_\n",
    "            true_positives_best_pre_flBFS[ct_outer,node] = true_positives_flBFS_\n",
    "            precision_best_pre_flBFS[ct_outer,node] = precision\n",
    "            recall_best_pre_flBFS[ct_outer,node] = recall\n",
    "            f1score_best_pre_flBFS[ct_outer,node] = f1_score_\n",
    "\n",
    "            cuts_best_pre_flBFS[ct_outer,node] = S\n",
    "\n",
    "        if cond_val_l1pr <= min_conduct:\n",
    "\n",
    "            min_conduct = cond_val_l1pr\n",
    "\n",
    "            external_best_cond_flBFS[ct_outer,node] = cond_val_l1pr\n",
    "            vol_best_cond_flBFS[ct_outer,node] = vol_\n",
    "\n",
    "            size_clust_best_cond_flBFS[ct_outer,node] = size_clust_flBFS_\n",
    "            true_positives_best_cond_flBFS[ct_outer,node] = true_positives_flBFS_\n",
    "            precision_best_cond_flBFS[ct_outer,node] = precision\n",
    "            recall_best_cond_flBFS[ct_outer,node] = recall\n",
    "            f1score_best_cond_flBFS[ct_outer,node] = f1_score_\n",
    "\n",
    "            cuts_best_cond_flBFS[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_flBFS[ct_outer,node], 'f1score: ', f1score_best_cond_flBFS[ct_outer,node], 'precision: ', precision_best_cond_flBFS[ct_outer,node], 'recall: ', recall_best_cond_flBFS[ct_outer,node])\n",
    "        ct += 1\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time BFS+SL: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of BFS+FlowImp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for BFS+SL')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_flBFS[i,j])\n",
    "        temp_rec.append(recall_best_cond_flBFS[i,j])\n",
    "        temp_f1.append(f1score_best_cond_flBFS[i,j])\n",
    "        temp_conductance.append(external_best_cond_flBFS[i,j])\n",
    "\n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for APPR+SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_apprSL = {}\n",
    "external_best_pre_cond_apprSL = {}\n",
    "vol_best_cond_apprSL = {}\n",
    "vol_best_pre_apprSL = {}\n",
    "size_clust_best_cond_apprSL = {}\n",
    "size_clust_best_pre_apprSL = {}\n",
    "f1score_best_cond_apprSL = {}\n",
    "f1score_best_pre_apprSL = {}\n",
    "true_positives_best_cond_apprSL = {}\n",
    "true_positives_best_pre_apprSL = {}\n",
    "precision_best_cond_apprSL = {}\n",
    "precision_best_pre_apprSL = {}\n",
    "recall_best_cond_apprSL = {}\n",
    "recall_best_pre_apprSL = {}\n",
    "cuts_best_cond_apprSL = {}\n",
    "cuts_best_pre_apprSL = {}\n",
    "cuts_apprSL_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    eigv, lambda_val = fiedler_local(g, rr)\n",
    "    lambda_val = np.real(lambda_val)\n",
    "    \n",
    "    step = (2*lambda_val - lambda_val/2)/4\n",
    "    \n",
    "    a_list = np.arange(lambda_val/2,2*lambda_val,step)\n",
    "    \n",
    "    vol_target = np.sum(g.d[rr])\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "        \n",
    "        ct_inner = 0\n",
    "        for a in a_list:\n",
    "            \n",
    "            if ct_outer <= 1:\n",
    "                rho = 0.15/np.sum(g.d[rr])\n",
    "            else:\n",
    "                rho = 0.2/np.sum(g.d[rr])\n",
    "            \n",
    "            output_pr_clustering = approximate_PageRank(g,ref_node,method = \"acl\", rho=rho, alpha=a, cpp = True, normalize=True,normalized_objective=True)\n",
    "            number_experiments += 1\n",
    "            \n",
    "            output_pr_sc = sweep_cut(g,output_pr_clustering,cpp=True)\n",
    "            \n",
    "            S = output_pr_sc[0]\n",
    "            \n",
    "            vol_input = np.sum(g.d[S])\n",
    "\n",
    "            vol_graph_minus_input = np.sum(g.d[list(set(range(g._num_vertices)) - set(S))])\n",
    "\n",
    "            vol_target_intersection_input = np.sum(g.d[list(set(rr).intersection(set(S)))])\n",
    "\n",
    "            gamma = vol_input/vol_graph_minus_input\n",
    "\n",
    "            sigma = max(vol_target_intersection_input/vol_target,gamma)\n",
    "\n",
    "            delta = min(max((1/3)*(1.0/(1.0/sigma - 1)) - gamma,0),1)\n",
    "\n",
    "            S = flow_clustering(g,S,method=\"sl\",delta=delta)[0]\n",
    "            \n",
    "            cuts_apprSL_ALL[ct_outer,node,ct_inner] = S\n",
    "            \n",
    "            size_clust_apprSL_ = len(S)\n",
    "            \n",
    "            cond_val_l1pr = g.compute_conductance(S)\n",
    "            \n",
    "            vol_ = sum(g.d[S])\n",
    "            true_positives_apprSL_ = set(rr).intersection(S)\n",
    "            if len(true_positives_apprSL_) == 0:\n",
    "                true_positives_apprSL_ = set(ref_node)\n",
    "                vol_ = g.d[ref_node][0]\n",
    "            precision = sum(g.d[np.array(list(true_positives_apprSL_))])/vol_\n",
    "            recall = sum(g.d[np.array(list(true_positives_apprSL_))])/sum(g.d[rr])\n",
    "            f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if f1_score_ >= max_precision:\n",
    "                \n",
    "                max_precision = f1_score_\n",
    "                \n",
    "                external_best_pre_cond_apprSL[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_pre_apprSL[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_pre_apprSL[ct_outer,node] = size_clust_apprSL_\n",
    "                true_positives_best_pre_apprSL[ct_outer,node] = true_positives_apprSL_\n",
    "                precision_best_pre_apprSL[ct_outer,node] = precision\n",
    "                recall_best_pre_apprSL[ct_outer,node] = recall\n",
    "                f1score_best_pre_apprSL[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_pre_apprSL[ct_outer,node] = S\n",
    "        \n",
    "            if cond_val_l1pr <= min_conduct:\n",
    "                \n",
    "                min_conduct = cond_val_l1pr\n",
    "                \n",
    "                external_best_cond_apprSL[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_cond_apprSL[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_cond_apprSL[ct_outer,node] = size_clust_apprSL_\n",
    "                true_positives_best_cond_apprSL[ct_outer,node] = true_positives_apprSL_\n",
    "                precision_best_cond_apprSL[ct_outer,node] = precision\n",
    "                recall_best_cond_apprSL[ct_outer,node] = recall\n",
    "                f1score_best_cond_apprSL[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_cond_apprSL[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_apprSL[ct_outer,node], 'f1score: ', f1score_best_cond_apprSL[ct_outer,node], 'precision: ', precision_best_cond_apprSL[ct_outer,node], 'recall: ', recall_best_cond_apprSL[ct_outer,node])\n",
    "        ct += 1\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time APPR+SL with rounding: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of APPR+SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for APPR+SL')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_apprSL[i,j])\n",
    "        temp_rec.append(recall_best_cond_apprSL[i,j])\n",
    "        temp_f1.append(f1score_best_cond_apprSL[i,j])\n",
    "        temp_conductance.append(external_best_cond_apprSL[i,j])\n",
    "\n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for L1+SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_l1SL = {}\n",
    "external_best_pre_cond_l1SL = {}\n",
    "vol_best_cond_l1SL = {}\n",
    "vol_best_pre_l1SL = {}\n",
    "size_clust_best_cond_l1SL = {}\n",
    "size_clust_best_pre_l1SL = {}\n",
    "f1score_best_cond_l1SL = {}\n",
    "f1score_best_pre_l1SL = {}\n",
    "true_positives_best_cond_l1SL = {}\n",
    "true_positives_best_pre_l1SL = {}\n",
    "precision_best_cond_l1SL = {}\n",
    "precision_best_pre_l1SL = {}\n",
    "recall_best_cond_l1SL = {}\n",
    "recall_best_pre_l1SL = {}\n",
    "cuts_best_cond_l1SL = {}\n",
    "cuts_best_pre_l1SL = {}\n",
    "cuts_l1SL_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    eigv, lambda_val = fiedler_local(g, rr)\n",
    "    lambda_val = np.real(lambda_val)\n",
    "    \n",
    "    step = (2*lambda_val - lambda_val/2)/4\n",
    "    \n",
    "    a_list = np.arange(lambda_val/2,2*lambda_val,step)\n",
    "    \n",
    "    vol_target = np.sum(g.d[rr])\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "        \n",
    "        ct_inner = 0\n",
    "        for a in a_list:\n",
    "            \n",
    "            if ct_outer <= 1:\n",
    "                rho = 0.15/np.sum(g.d[rr])\n",
    "            else:\n",
    "                rho = 0.2/np.sum(g.d[rr])\n",
    "            \n",
    "            output_pr_clustering = approximate_PageRank(g,ref_node,method = \"l1reg-rand\", epsilon=1.0e-6, rho=rho, alpha=a, cpp = True, normalize=True,normalized_objective=True)\n",
    "            number_experiments += 1\n",
    "            \n",
    "            output_pr_sc = sweep_cut(g,output_pr_clustering,cpp=True)\n",
    "            \n",
    "            S = output_pr_sc[0]\n",
    "            \n",
    "            vol_input = np.sum(g.d[S])\n",
    "\n",
    "            vol_graph_minus_input = np.sum(g.d[list(set(range(g._num_vertices)) - set(S))])\n",
    "\n",
    "            vol_target_intersection_input = np.sum(g.d[list(set(rr).intersection(set(S)))])\n",
    "\n",
    "            gamma = vol_input/vol_graph_minus_input\n",
    "\n",
    "            sigma = max(vol_target_intersection_input/vol_target,gamma)\n",
    "\n",
    "            delta = min(max((1/3)*(1.0/(1.0/sigma - 1)) - gamma,0),1)\n",
    "\n",
    "            S = flow_clustering(g,S,method=\"sl\",delta=delta)[0]\n",
    "            \n",
    "            cuts_l1SL_ALL[ct_outer,node,ct_inner] = S\n",
    "            \n",
    "            size_clust_l1SL_ = len(S)\n",
    "            \n",
    "            cond_val_l1pr = g.compute_conductance(S)\n",
    "            \n",
    "            vol_ = sum(g.d[S])\n",
    "            true_positives_l1SL_ = set(rr).intersection(S)\n",
    "            if len(true_positives_l1SL_) == 0:\n",
    "                true_positives_l1SL_ = set(ref_node)\n",
    "                vol_ = g.d[ref_node][0]\n",
    "            precision = sum(g.d[np.array(list(true_positives_l1SL_))])/vol_\n",
    "            recall = sum(g.d[np.array(list(true_positives_l1SL_))])/sum(g.d[rr])\n",
    "            f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if f1_score_ >= max_precision:\n",
    "                \n",
    "                max_precision = f1_score_\n",
    "                \n",
    "                external_best_pre_cond_l1SL[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_pre_l1SL[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_pre_l1SL[ct_outer,node] = size_clust_l1SL_\n",
    "                true_positives_best_pre_l1SL[ct_outer,node] = true_positives_l1SL_\n",
    "                precision_best_pre_l1SL[ct_outer,node] = precision\n",
    "                recall_best_pre_l1SL[ct_outer,node] = recall\n",
    "                f1score_best_pre_l1SL[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_pre_l1SL[ct_outer,node] = S\n",
    "        \n",
    "            if cond_val_l1pr <= min_conduct:\n",
    "                \n",
    "                min_conduct = cond_val_l1pr\n",
    "                \n",
    "                external_best_cond_l1SL[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_cond_l1SL[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_cond_l1SL[ct_outer,node] = size_clust_l1SL_\n",
    "                true_positives_best_cond_l1SL[ct_outer,node] = true_positives_l1SL_\n",
    "                precision_best_cond_l1SL[ct_outer,node] = precision\n",
    "                recall_best_cond_l1SL[ct_outer,node] = recall\n",
    "                f1score_best_cond_l1SL[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_cond_l1SL[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_l1SL[ct_outer,node], 'f1score: ', f1score_best_cond_l1SL[ct_outer,node], 'precision: ', precision_best_cond_l1SL[ct_outer,node], 'recall: ', recall_best_cond_l1SL[ct_outer,node])\n",
    "        ct += 1\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time L1+SL with rounding: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of l1+SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for L1+SL')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_l1SL[i,j])\n",
    "        temp_rec.append(recall_best_cond_l1SL[i,j])\n",
    "        temp_f1.append(f1score_best_cond_l1SL[i,j])\n",
    "        temp_conductance.append(external_best_cond_l1SL[i,j])\n",
    "\n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
