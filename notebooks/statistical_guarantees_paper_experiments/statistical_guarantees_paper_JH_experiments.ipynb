{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "try:\n",
    "    from localgraphclustering import *\n",
    "except:\n",
    "    # when the package is not installed, import the local version instead. \n",
    "    # the notebook must be placed in the original \"notebooks/\" folder\n",
    "    sys.path.append(\"../\")\n",
    "    from localgraphclustering import * \n",
    "\n",
    "import time\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "import random\n",
    "\n",
    "import statistics as stat_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/localgraphclustering/GraphLocal.py:222: UserWarning:\n",
      "\n",
      "Loading a graphml is not efficient, we suggest using an edgelist format for this API.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g = GraphLocal('../datasets/JohnsHopkins.graphml','graphml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.read_graphml('../datasets/JohnsHopkins.graphml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Id:  gender2.0\n",
      "Cluster:  3  conductance:  0.48658398838201056 Size:  2598  Volume:  173524.0\n",
      "Id:  year2008.0\n",
      "Cluster:  6  conductance:  0.39325246581619117 Size:  926  Volume:  82934.0\n",
      "Id:  year2009.0\n",
      "Cluster:  10  conductance:  0.21153089930124927 Size:  910  Volume:  33059.0\n",
      "Id:  year2007.0\n",
      "Cluster:  15  conductance:  0.4916255714943665 Size:  842  Volume:  89021.0\n",
      "Id:  gender1.0\n",
      "Cluster:  18  conductance:  0.4685889813713833 Size:  2144  Volume:  181656.0\n",
      "Id:  secondMajor0.0\n",
      "Cluster:  21  conductance:  0.5140141770673018 Size:  2844  Volume:  178034.0\n",
      "Id:  dorm0.0\n",
      "Cluster:  23  conductance:  0.5250280681801612 Size:  2121  Volume:  137166.0\n",
      "Id:  year2006.0\n",
      "Cluster:  25  conductance:  0.5489968617586363 Size:  845  Volume:  81893.0\n",
      "Id:  majorIndex217.0\n",
      "Cluster:  36  conductance:  0.2624100215013555 Size:  201  Volume:  10697.0\n"
     ]
    }
   ],
   "source": [
    "ground_truth_clusters_by_number = dict()\n",
    "\n",
    "cluster_names = ['secondMajor','highSchool','gender','dorm','majorIndex','year']\n",
    "\n",
    "for node in G.nodes(data=True):\n",
    "    \n",
    "    for cluster_name in cluster_names:\n",
    "        \n",
    "        ground_truth_clusters_by_number[cluster_name+str(node[1][cluster_name])] =  []\n",
    "\n",
    "counter = 0\n",
    "for node in G.nodes(data=True):\n",
    "    \n",
    "    for cluster_name in cluster_names:\n",
    "        \n",
    "        ground_truth_clusters_by_number[cluster_name+str(node[1][cluster_name])].append(counter)\n",
    "    counter += 1\n",
    "    \n",
    "all_clusters = []\n",
    "counter = 0\n",
    "for cluster_id in ground_truth_clusters_by_number:\n",
    "    \n",
    "    cluster = ground_truth_clusters_by_number[cluster_id]\n",
    "    \n",
    "    if len(cluster) == 1 or len(cluster) == 0:\n",
    "        counter += 1\n",
    "        continue\n",
    "    \n",
    "    cond = g.compute_conductance(cluster, cpp=True)\n",
    "    counter += 1\n",
    "    \n",
    "    if cond <= 0.57 and len(cluster) >= 10:\n",
    "        print(\"Id: \", cluster_id)\n",
    "        print(\"Cluster: \", counter, \" conductance: \", cond, \"Size: \", len(cluster), \" Volume: \", np.sum(g.d[cluster]))\n",
    "        all_clusters.append(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for l1-reg. PR (with rounding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2598\n",
      "2598\n",
      "outer: 0 number of node:  2041  completed:  0.0  degree:  24.0\n",
      "conductance:  0.630706932443608 f1score:  0.3029984650616849 precision:  0.5263353009952377 recall:  0.2127313800972776\n",
      "outer: 0 number of node:  2041  completed:  0.0  degree:  24.0\n",
      "conductance:  0.630706932443608 f1score:  0.3029984650616849 precision:  0.5263353009952377 recall:  0.2127313800972776\n",
      "outer: 0 number of node:  4243  completed:  0.00038491147036181676  degree:  107.0\n",
      "conductance:  0.19541444016403628 f1score:  0.15160029555883953 precision:  0.4844351932397167 recall:  0.08986076853922224\n",
      "outer: 0 number of node:  5088  completed:  0.0007698229407236335  degree:  14.0\n",
      "conductance:  0.5792609757106798 f1score:  0.11273148499425871 precision:  0.46108329540282206 recall:  0.06421590097047095\n",
      "outer: 0 number of node:  4243  completed:  0.00038491147036181676  degree:  107.0\n",
      "conductance:  0.19541444016403628 f1score:  0.15160029555883953 precision:  0.4844351932397167 recall:  0.08986076853922224\n",
      "outer: 0 number of node:  5088  completed:  0.0007698229407236335  degree:  14.0\n",
      "conductance:  0.5792609757106798 f1score:  0.11273148499425871 precision:  0.46108329540282206 recall:  0.06421590097047095\n",
      "outer: 0 number of node:  3795  completed:  0.0011547344110854503  degree:  49.0\n",
      "conductance:  0.2964012711500472 f1score:  0.4344602181139039 precision:  0.4195761401700593 recall:  0.4504391323390424\n",
      "outer: 0 number of node:  3795  completed:  0.0011547344110854503  degree:  49.0\n",
      "conductance:  0.2964012711500472 f1score:  0.4344602181139039 precision:  0.4195761401700593 recall:  0.4504391323390424\n",
      "outer: 0 number of node:  3611  completed:  0.001539645881447267  degree:  63.0\n",
      "conductance:  0.36963201773165755 f1score:  0.41225145635742144 precision:  0.4313464977457623 recall:  0.3947753624858809\n",
      "outer: 0 number of node:  3611  completed:  0.001539645881447267  degree:  63.0\n",
      "conductance:  0.36963201773165755 f1score:  0.41225145635742144 precision:  0.4313464977457623 recall:  0.3947753624858809\n",
      "outer: 0 number of node:  4955  completed:  0.001924557351809084  degree:  424.0\n",
      "conductance:  0.23918916021696718 f1score:  0.48374877810361677 precision:  0.4668331796839826 recall:  0.5019363315737304\n",
      "outer: 0 number of node:  4955  completed:  0.001924557351809084  degree:  424.0\n",
      "conductance:  0.23918916021696718 f1score:  0.48374877810361677 precision:  0.4668331796839826 recall:  0.5019363315737304\n",
      "outer: 0 number of node:  4485  completed:  0.0023094688221709007  degree:  121.0\n",
      "conductance:  0.3154986197839895 f1score:  0.4634076575513276 precision:  0.4472114276525608 recall:  0.48082109679352714\n",
      "outer: 0 number of node:  4485  completed:  0.0023094688221709007  degree:  121.0\n",
      "conductance:  0.3154986197839895 f1score:  0.4634076575513276 precision:  0.4472114276525608 recall:  0.48082109679352714\n",
      "outer: 0 number of node:  2573  completed:  0.0026943802925327174  degree:  82.0\n",
      "conductance:  0.2665229885057471 f1score:  0.16688283202044668 precision:  0.48350464190981435 recall:  0.10084483990687167\n",
      "outer: 0 number of node:  2573  completed:  0.0026943802925327174  degree:  82.0\n",
      "conductance:  0.2665229885057471 f1score:  0.16688283202044668 precision:  0.48350464190981435 recall:  0.10084483990687167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-16bca4e10403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0moutput_pr_clustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapproximate_PageRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"l1reg-rand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalized_objective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mnumber_experiments\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/localgraphclustering/approximate_PageRank.py\u001b[0m in \u001b[0;36mapproximate_PageRank\u001b[0;34m(G, ref_nodes, timeout, iterations, alpha, rho, epsilon, ys, cpp, normalize, normalized_objective, method)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 (length,xids,values) = algo(G.ai, G.aj, G.adjacency_matrix.data, ref_nodes, G.d, G.d_sqrt, G.dn_sqrt, alpha = alpha,\n\u001b[0;32m--> 169\u001b[0;31m                                      rho = rho, epsilon = epsilon, maxiter = iterations, max_time = timeout, normalized_objective = normalized_objective)\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfista_dinput_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/localgraphclustering/cpp/proxl1PRrand_cpp.py\u001b[0m in \u001b[0;36mproxl1PRrand_cpp\u001b[0;34m(ai, aj, a, ref_node, d, ds, dsinv, alpha, rho, epsilon, maxiter, max_time, normalized_objective)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#     start2 = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mactual_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdsinv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_objective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m#     end2 = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-16bca4e10403>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.15\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0moutput_pr_clustering\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapproximate_PageRank\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"l1reg-rand\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0e-2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcpp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnormalized_objective\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mnumber_experiments\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/localgraphclustering/approximate_PageRank.py\u001b[0m in \u001b[0;36mapproximate_PageRank\u001b[0;34m(G, ref_nodes, timeout, iterations, alpha, rho, epsilon, ys, cpp, normalize, normalized_objective, method)\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 (length,xids,values) = algo(G.ai, G.aj, G.adjacency_matrix.data, ref_nodes, G.d, G.d_sqrt, G.dn_sqrt, alpha = alpha,\n\u001b[0;32m--> 169\u001b[0;31m                                      rho = rho, epsilon = epsilon, maxiter = iterations, max_time = timeout, normalized_objective = normalized_objective)\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfista_dinput_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_nodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miterations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/localgraphclustering/cpp/proxl1PRrand_cpp.py\u001b[0m in \u001b[0;36mproxl1PRrand_cpp\u001b[0;34m(ai, aj, a, ref_node, d, ds, dsinv, alpha, rho, epsilon, maxiter, max_time, normalized_objective)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;31m#     start2 = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m     \u001b[0mactual_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mai\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrho\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_node\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdsinv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxiter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_objective\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;31m#     end2 = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nodes = {}\n",
    "external_best_cond_acl = {}\n",
    "external_best_pre_cond_acl = {}\n",
    "vol_best_cond_acl = {}\n",
    "vol_best_pre_acl = {}\n",
    "size_clust_best_cond_acl = {}\n",
    "size_clust_best_pre_acl = {}\n",
    "f1score_best_cond_acl = {}\n",
    "f1score_best_pre_acl = {}\n",
    "true_positives_best_cond_acl = {}\n",
    "true_positives_best_pre_acl = {}\n",
    "precision_best_cond_acl = {}\n",
    "precision_best_pre_acl = {}\n",
    "recall_best_cond_acl = {}\n",
    "recall_best_pre_acl = {}\n",
    "cuts_best_cond_acl = {}\n",
    "cuts_best_pre_acl = {}\n",
    "cuts_acl_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    eigv, lambda_val = fiedler_local(g, rr)\n",
    "    lambda_val = np.real(lambda_val)\n",
    "    \n",
    "    step = (2*lambda_val - lambda_val/2)/4\n",
    "    \n",
    "    a_list = np.arange(lambda_val/2,2*lambda_val,step)\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "        \n",
    "        ct_inner = 0\n",
    "        for a in a_list:\n",
    "            \n",
    "            if ct_outer <= 1:\n",
    "                rho = 0.1/np.sum(g.d[rr])\n",
    "            else:\n",
    "                rho = 0.15/np.sum(g.d[rr])\n",
    "            \n",
    "            output_pr_clustering = approximate_PageRank(g,ref_node,method = \"l1reg-rand\", epsilon=1.0e-2, rho=rho, alpha=a, cpp = True, normalize=True,normalized_objective=True)\n",
    "            number_experiments += 1\n",
    "            \n",
    "            output_pr_sc = sweep_cut(g,output_pr_clustering,cpp=True)\n",
    "            \n",
    "            S = output_pr_sc[0]\n",
    "            \n",
    "            cuts_acl_ALL[ct_outer,node,ct_inner] = S\n",
    "            \n",
    "            size_clust_acl_ = len(S)\n",
    "            \n",
    "            cond_val_l1pr = g.compute_conductance(S)\n",
    "            \n",
    "            vol_ = sum(g.d[S])\n",
    "            true_positives_acl_ = set(rr).intersection(S)\n",
    "            if len(true_positives_acl_) == 0:\n",
    "                true_positives_acl_ = set(ref_node)\n",
    "                vol_ = g.d[ref_node][0,0]\n",
    "            precision = sum(g.d[np.array(list(true_positives_acl_))])/vol_\n",
    "            recall = sum(g.d[np.array(list(true_positives_acl_))])/sum(g.d[rr])\n",
    "            f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if f1_score_ >= max_precision:\n",
    "                \n",
    "                max_precision = f1_score_\n",
    "                \n",
    "                external_best_pre_cond_acl[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_pre_acl[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_pre_acl[ct_outer,node] = size_clust_acl_\n",
    "                true_positives_best_pre_acl[ct_outer,node] = true_positives_acl_\n",
    "                precision_best_pre_acl[ct_outer,node] = precision\n",
    "                recall_best_pre_acl[ct_outer,node] = recall\n",
    "                f1score_best_pre_acl[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_pre_acl[ct_outer,node] = S\n",
    "        \n",
    "            if cond_val_l1pr <= min_conduct:\n",
    "                \n",
    "                min_conduct = cond_val_l1pr\n",
    "                \n",
    "                external_best_cond_acl[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_cond_acl[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_cond_acl[ct_outer,node] = size_clust_acl_\n",
    "                true_positives_best_cond_acl[ct_outer,node] = true_positives_acl_\n",
    "                precision_best_cond_acl[ct_outer,node] = precision\n",
    "                recall_best_cond_acl[ct_outer,node] = recall\n",
    "                f1score_best_cond_acl[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_cond_acl[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_acl[ct_outer,node], 'f1score: ', f1score_best_cond_acl[ct_outer,node], 'precision: ', precision_best_cond_acl[ct_outer,node], 'recall: ', recall_best_cond_acl[ct_outer,node])\n",
    "        ct += 1\n",
    "\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time l1-reg. with rounding: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of l1-reg. PR (with rounding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for l1-reg with rounding')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_acl[i,j])\n",
    "        temp_rec.append(recall_best_cond_acl[i,j])\n",
    "        temp_f1.append(f1score_best_cond_acl[i,j])\n",
    "        temp_conductance.append(external_best_cond_acl[i,j])\n",
    "    \n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for seed set expansion using BFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import queue\n",
    "def seed_grow_bfs_steps(g,seeds,steps,vol_target,target_cluster):\n",
    "    \"\"\"\n",
    "    grow the initial seed set through BFS until its size reaches \n",
    "    a given ratio of the total number of nodes.\n",
    "    \"\"\"\n",
    "    Q = queue.Queue()\n",
    "    visited = np.zeros(g._num_vertices)\n",
    "    visited[seeds] = 1\n",
    "    for s in seeds:\n",
    "        Q.put(s)\n",
    "    if isinstance(seeds,np.ndarray):\n",
    "        seeds = seeds.tolist()\n",
    "    else:\n",
    "        seeds = list(seeds)\n",
    "    for step in range(steps):\n",
    "        for k in range(Q.qsize()):\n",
    "            node = Q.get()\n",
    "            si,ei = g.adjacency_matrix.indptr[node],g.adjacency_matrix.indptr[node+1]\n",
    "            neighs = g.adjacency_matrix.indices[si:ei]\n",
    "            for i in range(len(neighs)):\n",
    "                if visited[neighs[i]] == 0:\n",
    "                    visited[neighs[i]] = 1\n",
    "                    seeds.append(neighs[i])\n",
    "                    Q.put(neighs[i])\n",
    "                    \n",
    "                    vol_seeds = np.sum(g.d[seeds])\n",
    "                    vol_target_intersection_input = np.sum(g.d[list(set(target_cluster).intersection(set(seeds)))])\n",
    "                    sigma = vol_target_intersection_input/vol_target\n",
    "                    \n",
    "                    if sigma > 0.75 or vol_seeds > 0.25*g.vol_G:\n",
    "                        break\n",
    "                 \n",
    "            vol_seeds = np.sum(g.d[seeds])\n",
    "            vol_target_intersection_input = np.sum(g.d[list(set(target_cluster).intersection(set(seeds)))])\n",
    "            sigma = vol_target_intersection_input/vol_target   \n",
    "            \n",
    "            if sigma > 0.75 or vol_seeds > 0.25*g.vol_G:\n",
    "                break\n",
    "               \n",
    "        vol_seeds = np.sum(g.d[seeds])\n",
    "        vol_target_intersection_input = np.sum(g.d[list(set(target_cluster).intersection(set(seeds)))])\n",
    "        sigma = vol_target_intersection_input/vol_target\n",
    "                \n",
    "        if sigma > 0.75 or vol_seeds > 0.25*g.vol_G:\n",
    "            break\n",
    "    return seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for seed set expansion + FlowImprove, try a lot of parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_flBFS = {}\n",
    "external_best_pre_cond_flBFS = {}\n",
    "vol_best_cond_flBFS = {}\n",
    "vol_best_pre_flBFS = {}\n",
    "size_clust_best_cond_flBFS = {}\n",
    "size_clust_best_pre_flBFS = {}\n",
    "f1score_best_cond_flBFS = {}\n",
    "f1score_best_pre_flBFS = {}\n",
    "true_positives_best_cond_flBFS = {}\n",
    "true_positives_best_pre_flBFS = {}\n",
    "precision_best_cond_flBFS = {}\n",
    "precision_best_pre_flBFS = {}\n",
    "recall_best_cond_flBFS = {}\n",
    "recall_best_pre_flBFS = {}\n",
    "cuts_best_cond_flBFS = {}\n",
    "cuts_best_pre_flBFS = {}\n",
    "cuts_flBFS_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    n_step = 24\n",
    "    \n",
    "    vol_target = np.sum(g.d[rr])\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "                \n",
    "        seeds = seed_grow_bfs_steps(g,[node],g._num_vertices,vol_target,rr)\n",
    "\n",
    "        vol_input = np.sum(g.d[seeds])\n",
    "\n",
    "        vol_graph_minus_input = np.sum(g.d[list(set(range(g._num_vertices)) - set(seeds))])\n",
    "\n",
    "        vol_target_intersection_input = np.sum(g.d[list(set(rr).intersection(set(seeds)))])\n",
    "\n",
    "        gamma = vol_input/vol_graph_minus_input\n",
    "                \n",
    "        sigma = max(vol_target_intersection_input/vol_target,gamma)\n",
    "        \n",
    "        delta = min(max((1/3)*(1.0/(1.0/sigma - 1)) - gamma,0),1)\n",
    "                        \n",
    "        S = flow_clustering(g,seeds,method=\"sl\",delta=delta)[0]\n",
    "        number_experiments += 1\n",
    "\n",
    "        cuts_flBFS_ALL[ct_outer,node] = S\n",
    "\n",
    "        size_clust_flBFS_ = len(S)\n",
    "\n",
    "        cond_val_l1pr = g.compute_conductance(S)\n",
    "\n",
    "        vol_ = sum(g.d[S])\n",
    "        true_positives_flBFS_ = set(rr).intersection(S)\n",
    "        if len(true_positives_flBFS_) == 0:\n",
    "            true_positives_flBFS_ = set(ref_node)\n",
    "            vol_ = g.d[ref_node][0]\n",
    "        precision = sum(g.d[np.array(list(true_positives_flBFS_))])/vol_\n",
    "        recall = sum(g.d[np.array(list(true_positives_flBFS_))])/sum(g.d[rr])\n",
    "        f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "\n",
    "        if f1_score_ >= max_precision:\n",
    "\n",
    "            max_precision = f1_score_\n",
    "\n",
    "            external_best_pre_cond_flBFS[ct_outer,node] = cond_val_l1pr\n",
    "            vol_best_pre_flBFS[ct_outer,node] = vol_\n",
    "\n",
    "            size_clust_best_pre_flBFS[ct_outer,node] = size_clust_flBFS_\n",
    "            true_positives_best_pre_flBFS[ct_outer,node] = true_positives_flBFS_\n",
    "            precision_best_pre_flBFS[ct_outer,node] = precision\n",
    "            recall_best_pre_flBFS[ct_outer,node] = recall\n",
    "            f1score_best_pre_flBFS[ct_outer,node] = f1_score_\n",
    "\n",
    "            cuts_best_pre_flBFS[ct_outer,node] = S\n",
    "\n",
    "        if cond_val_l1pr <= min_conduct:\n",
    "\n",
    "            min_conduct = cond_val_l1pr\n",
    "\n",
    "            external_best_cond_flBFS[ct_outer,node] = cond_val_l1pr\n",
    "            vol_best_cond_flBFS[ct_outer,node] = vol_\n",
    "\n",
    "            size_clust_best_cond_flBFS[ct_outer,node] = size_clust_flBFS_\n",
    "            true_positives_best_cond_flBFS[ct_outer,node] = true_positives_flBFS_\n",
    "            precision_best_cond_flBFS[ct_outer,node] = precision\n",
    "            recall_best_cond_flBFS[ct_outer,node] = recall\n",
    "            f1score_best_cond_flBFS[ct_outer,node] = f1_score_\n",
    "\n",
    "            cuts_best_cond_flBFS[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_flBFS[ct_outer,node], 'f1score: ', f1score_best_cond_flBFS[ct_outer,node], 'precision: ', precision_best_cond_flBFS[ct_outer,node], 'recall: ', recall_best_cond_flBFS[ct_outer,node])\n",
    "        ct += 1\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time BFS+SL: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of BFS+FlowImp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for BFS+SL')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_flBFS[i,j])\n",
    "        temp_rec.append(recall_best_cond_flBFS[i,j])\n",
    "        temp_f1.append(f1score_best_cond_flBFS[i,j])\n",
    "        temp_conductance.append(external_best_cond_flBFS[i,j])\n",
    "\n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect data for L1+SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = {}\n",
    "external_best_cond_l1SL = {}\n",
    "external_best_pre_cond_l1SL = {}\n",
    "vol_best_cond_l1SL = {}\n",
    "vol_best_pre_l1SL = {}\n",
    "size_clust_best_cond_l1SL = {}\n",
    "size_clust_best_pre_l1SL = {}\n",
    "f1score_best_cond_l1SL = {}\n",
    "f1score_best_pre_l1SL = {}\n",
    "true_positives_best_cond_l1SL = {}\n",
    "true_positives_best_pre_l1SL = {}\n",
    "precision_best_cond_l1SL = {}\n",
    "precision_best_pre_l1SL = {}\n",
    "recall_best_cond_l1SL = {}\n",
    "recall_best_pre_l1SL = {}\n",
    "cuts_best_cond_l1SL = {}\n",
    "cuts_best_pre_l1SL = {}\n",
    "cuts_l1SL_ALL = {}\n",
    "\n",
    "ct_outer = 0\n",
    "\n",
    "number_experiments = 0\n",
    "\n",
    "for rr in all_clusters:\n",
    "    \n",
    "    how_many = int(len(rr))\n",
    "    print(how_many)\n",
    "    \n",
    "    random.seed(4)\n",
    "    \n",
    "    nodes[ct_outer] = np.random.choice(rr, how_many, replace=False)\n",
    "    \n",
    "    eigv, lambda_val = fiedler_local(g, rr)\n",
    "    lambda_val = np.real(lambda_val)\n",
    "    \n",
    "    step = (2*lambda_val - lambda_val/2)/4\n",
    "    \n",
    "    a_list = np.arange(lambda_val/2,2*lambda_val,step)\n",
    "    \n",
    "    vol_target = np.sum(g.d[rr])\n",
    "    \n",
    "    ct = 0\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    for node in nodes[ct_outer]:\n",
    "        ref_node = [node]\n",
    "        \n",
    "        max_precision = -1\n",
    "        min_conduct = 100\n",
    "        \n",
    "        ct_inner = 0\n",
    "        for a in a_list:\n",
    "            \n",
    "            if ct_outer <= 1:\n",
    "                rho = 0.1/np.sum(g.d[rr])\n",
    "            else:\n",
    "                rho = 0.15/np.sum(g.d[rr])\n",
    "            \n",
    "            output_pr_clustering = approximate_PageRank(g,ref_node,method = \"l1reg-rand\", epsilon=1.0e-2, rho=rho, alpha=a, cpp = True, normalize=True,normalized_objective=True)\n",
    "            number_experiments += 1\n",
    "            \n",
    "            output_pr_sc = sweep_cut(g,output_pr_clustering,cpp=True)\n",
    "            \n",
    "            S = output_pr_sc[0]\n",
    "            \n",
    "            vol_input = np.sum(g.d[S])\n",
    "\n",
    "            vol_graph_minus_input = np.sum(g.d[list(set(range(g._num_vertices)) - set(S))])\n",
    "\n",
    "            vol_target_intersection_input = np.sum(g.d[list(set(rr).intersection(set(S)))])\n",
    "\n",
    "            gamma = vol_input/vol_graph_minus_input\n",
    "\n",
    "            sigma = max(vol_target_intersection_input/vol_target,gamma)\n",
    "\n",
    "            delta = min(max((1/3)*(1.0/(1.0/sigma - 1)) - gamma,0),1)\n",
    "\n",
    "            S = flow_clustering(g,S,method=\"sl\",delta=delta)[0]\n",
    "            \n",
    "            cuts_l1SL_ALL[ct_outer,node,ct_inner] = S\n",
    "            \n",
    "            size_clust_l1SL_ = len(S)\n",
    "            \n",
    "            cond_val_l1pr = g.compute_conductance(S)\n",
    "            \n",
    "            vol_ = sum(g.d[S])\n",
    "            true_positives_l1SL_ = set(rr).intersection(S)\n",
    "            if len(true_positives_l1SL_) == 0:\n",
    "                true_positives_l1SL_ = set(ref_node)\n",
    "                vol_ = g.d[ref_node][0]\n",
    "            precision = sum(g.d[np.array(list(true_positives_l1SL_))])/vol_\n",
    "            recall = sum(g.d[np.array(list(true_positives_l1SL_))])/sum(g.d[rr])\n",
    "            f1_score_ = 2*(precision*recall)/(precision + recall)\n",
    "            \n",
    "            if f1_score_ >= max_precision:\n",
    "                \n",
    "                max_precision = f1_score_\n",
    "                \n",
    "                external_best_pre_cond_l1SL[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_pre_l1SL[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_pre_l1SL[ct_outer,node] = size_clust_l1SL_\n",
    "                true_positives_best_pre_l1SL[ct_outer,node] = true_positives_l1SL_\n",
    "                precision_best_pre_l1SL[ct_outer,node] = precision\n",
    "                recall_best_pre_l1SL[ct_outer,node] = recall\n",
    "                f1score_best_pre_l1SL[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_pre_l1SL[ct_outer,node] = S\n",
    "        \n",
    "            if cond_val_l1pr <= min_conduct:\n",
    "                \n",
    "                min_conduct = cond_val_l1pr\n",
    "                \n",
    "                external_best_cond_l1SL[ct_outer,node] = cond_val_l1pr\n",
    "                vol_best_cond_l1SL[ct_outer,node] = vol_\n",
    "                \n",
    "                size_clust_best_cond_l1SL[ct_outer,node] = size_clust_l1SL_\n",
    "                true_positives_best_cond_l1SL[ct_outer,node] = true_positives_l1SL_\n",
    "                precision_best_cond_l1SL[ct_outer,node] = precision\n",
    "                recall_best_cond_l1SL[ct_outer,node] = recall\n",
    "                f1score_best_cond_l1SL[ct_outer,node] = f1_score_\n",
    "                \n",
    "                cuts_best_cond_l1SL[ct_outer,node] = S\n",
    "\n",
    "        print('outer:', ct_outer, 'number of node: ',node, ' completed: ', ct/how_many, ' degree: ', g.d[node])\n",
    "        print('conductance: ', external_best_cond_l1SL[ct_outer,node], 'f1score: ', f1score_best_cond_l1SL[ct_outer,node], 'precision: ', precision_best_cond_l1SL[ct_outer,node], 'recall: ', recall_best_cond_l1SL[ct_outer,node])\n",
    "        ct += 1\n",
    "    end = time.time()\n",
    "    print(\" \")\n",
    "    print(\"Outer: \", ct_outer,\" Elapsed time L1+SL with rounding: \", end - start)\n",
    "    print(\"Outer: \", ct_outer,\" Number of experiments: \", number_experiments)\n",
    "    print(\" \")\n",
    "    ct_outer += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance of l1+SL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "xlabels_ = []\n",
    "\n",
    "print('Results for L1+SL')\n",
    "sum_precision = 0\n",
    "sum_recall = 0\n",
    "sum_f1 = 0\n",
    "sum_conductance = 0\n",
    "\n",
    "info_ref_nodes = all_clusters\n",
    "l_info_ref_nodes = len(info_ref_nodes)\n",
    "\n",
    "for i in range(l_info_ref_nodes):\n",
    "    temp_pre = []\n",
    "    temp_rec = []\n",
    "    temp_f1 = []\n",
    "    temp_conductance = []\n",
    "    \n",
    "    for j in all_clusters[i]:\n",
    "        temp_pre.append(precision_best_cond_l1SL[i,j])\n",
    "        temp_rec.append(recall_best_cond_l1SL[i,j])\n",
    "        temp_f1.append(f1score_best_cond_l1SL[i,j])\n",
    "        temp_conductance.append(external_best_cond_l1SL[i,j])\n",
    "\n",
    "    print('Feature:', i,'Precision', stat_.mean(temp_pre), 'Recall', stat_.mean(temp_rec), 'F1', stat_.mean(temp_f1), 'Cond.', stat_.mean(temp_conductance))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
